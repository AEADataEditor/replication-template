#!/usr/bin/env python3
"""
Download files from World Bank Reproducible Research Repository.

Version: 1.0.0

This script downloads the replication package, reproducibility verification report,
and README from a World Bank Reproducible Research Repository record using the
catalog API. It's designed for replication workflows where researchers need
to download published datasets, code, and verification materials.

Usage:
    python3 tools/download_worldbank.py DOI_OR_ID

Examples:
    # Using full DOI
    python3 tools/download_worldbank.py https://doi.org/10.60572/101y-vn15
    python3 tools/download_worldbank.py 10.60572/101y-vn15
    
    # Using DOI suffix only
    python3 tools/download_worldbank.py 101y-vn15

This script will:
1. Parse DOI to extract the suffix (e.g., "101y-vn15")
2. Resolve the DOI to find the World Bank catalog ID
3. Discover available download files from the catalog page
4. Download and appropriately name files:
   - README.pdf
   - reproducibility-wb-[DOI_SUFFIX].[DATE].pdf
   - wb-[DOI_SUFFIX].zip (unzipped to wb-[DOI_SUFFIX]/)
5. Create a directory structure: wb-[DOI_SUFFIX]/

Features:
- Automatic DOI parsing and resolution
- Dynamic discovery of download files
- Proper file naming conventions
- Automatic zip extraction
- Progress indicators
- Smart file type detection
"""

import argparse
import hashlib
import os
import re
import sys
import threading
import time
import zipfile
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse

import requests

# Version information
__version__ = "1.0.0"

class Spinner:
    """Progress spinner with download information."""
    def __init__(self, message="Loading", total_size=0):
        self.spinner_chars = ['‚£æ', '‚£Ω', '‚£ª', '‚¢ø', '‚°ø', '‚£ü', '‚£Ø', '‚£∑']  # Braille block spinner animation frames
        self.message = message
        self.total_size = total_size
        self.downloaded_size = 0
        self.running = False
        self.thread = None
        self.idx = 0
    
    def update_progress(self, downloaded_size):
        """Update the download progress."""
        self.downloaded_size = downloaded_size
    
    def format_bytes(self, bytes_val):
        """Format bytes into human readable format."""
        if bytes_val < 1024:
            return f"{bytes_val} B"
        elif bytes_val < 1024**2:
            return f"{bytes_val/1024:.1f} KB"
        elif bytes_val < 1024**3:
            return f"{bytes_val/(1024**2):.1f} MB"
        else:
            return f"{bytes_val/(1024**3):.1f} GB"
    
    def spin(self):
        while self.running:
            char = self.spinner_chars[self.idx % len(self.spinner_chars)]
            progress_info = f"Downloaded: {self.format_bytes(self.downloaded_size)}"
            
            if self.total_size > 0:
                percentage = (self.downloaded_size / self.total_size) * 100
                progress_info += f" of {self.format_bytes(self.total_size)} ({percentage:.1f}%)"
            
            print(f"\r{char} {progress_info}", end="", flush=True)
            self.idx += 1
            time.sleep(0.1)
    
    def start(self):
        # Don't start spinner in CI environments
        is_ci = os.getenv("CI")
        if not is_ci:
            self.running = True
            self.thread = threading.Thread(target=self.spin)
            self.thread.start()
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join()
        print("\r" + " " * (len(self.message) + 10), end="")  # Clear the line
        print("\r", end="", flush=True)

def extract_doi_suffix(input_str):
    """Extract DOI suffix from various input formats."""
    # Remove any trailing slashes
    input_str = input_str.rstrip('/')
    
    # Handle different input formats
    if re.match(r'^[a-zA-Z0-9]{4}-[a-zA-Z0-9]{4}$', input_str):
        # Direct DOI suffix (e.g., "101y-vn15")
        return input_str
    elif 'doi.org' in input_str:
        # Extract from DOI URL (e.g., "https://doi.org/10.60572/101y-vn15")
        match = re.search(r'/([a-zA-Z0-9]{4}-[a-zA-Z0-9]{4})$', input_str)
        if match:
            return match.group(1)
    elif '10.60572/' in input_str:
        # Extract from DOI format (e.g., "10.60572/101y-vn15")
        match = re.search(r'10\.60572/([a-zA-Z0-9]{4}-[a-zA-Z0-9]{4})', input_str)
        if match:
            return match.group(1)
    else:
        # Fallback: try to extract DOI suffix pattern at the end
        match = re.search(r'([a-zA-Z0-9]{4}-[a-zA-Z0-9]{4})$', input_str)
        if match:
            return match.group(1)
    
    raise ValueError(f"Could not extract DOI suffix from: {input_str}")

def resolve_catalog_info(doi_suffix):
    """Resolve DOI to get catalog ID and discover download IDs."""
    full_doi = f"https://doi.org/10.60572/{doi_suffix}"
    
    print(f"üåê Resolving DOI: {full_doi}")
    
    spinner = Spinner("Resolving DOI")
    spinner.start()
    
    try:
        # Follow all redirects manually to see the sequence
        session = requests.Session()
        
        # Add headers to mimic a real browser
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        current_url = full_doi
        redirect_count = 0
        max_redirects = 10
        
        while redirect_count < max_redirects:
            response = session.get(current_url, allow_redirects=False)
            
            if response.status_code in (301, 302, 303, 307, 308):
                # Follow redirect
                redirect_count += 1
                new_url = response.headers.get('Location')
                if not new_url:
                    break
                
                # Handle relative URLs
                if new_url.startswith('/'):
                    from urllib.parse import urljoin
                    new_url = urljoin(current_url, new_url)
                
                print(f"üîÑ Redirect {redirect_count}: {current_url} -> {new_url}")
                current_url = new_url
            elif response.status_code == 200:
                # Final destination reached
                break
            else:
                raise Exception(f"HTTP {response.status_code} at {current_url}")
        
        if redirect_count >= max_redirects:
            raise Exception("Too many redirects")
        
        # Now get the final page content
        final_response = session.get(current_url)
        if final_response.status_code != 200:
            raise Exception(f"HTTP {final_response.status_code} at final URL: {current_url}")
        
        spinner.stop()
        
        final_url = current_url
        print(f"‚úÖ Final URL: {final_url}")
        
        # Handle the newer study URL format - check for HTTP Refresh header
        if '/catalog/study/' in final_url:
            print("üîç Detected study URL format, checking for HTTP refresh redirect...")
            
            # Check for HTTP Refresh header (this is the key!)
            refresh_header = final_response.headers.get('Refresh', '')
            if refresh_header:
                print(f"‚úÖ Found HTTP Refresh header: {refresh_header}")
                
                # Parse the refresh header to extract the URL
                # Format is typically: "0;url=https://..."
                refresh_match = re.search(r'url=(.+)', refresh_header)
                if refresh_match:
                    catalog_url = refresh_match.group(1)
                    print(f"üîÑ Following HTTP refresh to: {catalog_url}")
                    
                    # Follow the refresh redirect
                    final_response = session.get(catalog_url)
                    if final_response.status_code == 200:
                        final_url = catalog_url
                        print(f"‚úÖ Successfully redirected to catalog: {final_url}")
                    else:
                        raise Exception(f"Failed to access catalog URL: {catalog_url} (HTTP {final_response.status_code})")
                else:
                    raise Exception(f"Could not parse refresh header: {refresh_header}")
            else:
                print("‚ö†Ô∏è  No HTTP Refresh header found, trying other methods...")
                
                # Fallback: try to get page content and look for meta refresh or other redirects
                page_content = final_response.text
                
                # Look for meta refresh in HTML
                meta_refresh = re.search(r'<meta[^>]*http-equiv=["\']refresh["\'][^>]*content=["\'][^"\']*url=([^"\']*)["\'][^>]*>', page_content, re.IGNORECASE)
                if meta_refresh:
                    redirect_url = meta_refresh.group(1)
                    print(f"üîÑ Found meta refresh: {redirect_url}")
                    if redirect_url.startswith('/'):
                        from urllib.parse import urljoin
                        redirect_url = urljoin(final_url, redirect_url)
                    
                    final_response = session.get(redirect_url)
                    final_url = redirect_url
                    print(f"‚úÖ Redirected via meta refresh to: {final_url}")
                else:
                    # Extract study ID and try to construct catalog URL
                    study_match = re.search(r'/catalog/study/([^/?]+)', final_url)
                    if study_match:
                        study_id = study_match.group(1)
                        print(f"üìã Study ID: {study_id}")
                        print("‚ö†Ô∏è  No automatic redirect found - this may require manual mapping")
                        raise Exception(f"Could not automatically resolve study URL to catalog URL for study ID: {study_id}")
                    else:
                        raise Exception("Could not extract study ID from URL")
        
        # Get the page content to extract download information
        page_content = final_response.text
        
        # Debug: check if page is empty
        if not page_content.strip():
            raise Exception("Empty page content received")
        
        # Try multiple patterns to extract catalog ID
        catalog_id = None
        
        # Pattern 1: Look for download links in the page
        catalog_match = re.search(r'catalog/(\d+)/download', page_content)
        if catalog_match:
            catalog_id = catalog_match.group(1)
        
        # Pattern 2: Extract from URL if it contains catalog ID
        if not catalog_id:
            url_match = re.search(r'catalog/(\d+)', final_url)
            if url_match:
                catalog_id = url_match.group(1)
        
        # Pattern 3: Look for catalog references in the HTML
        if not catalog_id:
            catalog_refs = re.findall(r'catalog[/=](\d+)', page_content)
            if catalog_refs:
                catalog_id = catalog_refs[0]  # Take the first one found
        
        if not catalog_id:
            # Debug: show more information
            print("DEBUG: Final URL:", final_url)
            print("DEBUG: Page content length:", len(page_content))
            print("DEBUG: First 1000 characters of page:")
            print(repr(page_content[:1000]))
            print("DEBUG: Looking for 'catalog' in content...")
            catalog_occurrences = [m.start() for m in re.finditer('catalog', page_content.lower())]
            print(f"Found 'catalog' at positions: {catalog_occurrences[:10]}...")
            raise Exception("Could not extract catalog ID from page content")
        
        print(f"üì¶ Catalog ID: {catalog_id}")
        
        # Extract download IDs by looking for download links in HTML
        download_pattern = rf'catalog/{catalog_id}/download/(\d+)'
        html_download_ids = list(set(re.findall(download_pattern, page_content)))
        
        print(f"üîç Found download IDs in HTML: {html_download_ids}")
        
        # The World Bank catalog may have additional download files not linked in HTML
        # Check for common missing files (like verification reports) in a targeted way
        print("üîç Checking for additional download files not linked in HTML...")
        
        all_download_ids = set(html_download_ids)
        
        if html_download_ids:
            # Get the range of IDs to check based on found IDs (but keep it narrow)
            min_id = min(int(id) for id in html_download_ids)
            max_id = max(int(id) for id in html_download_ids)
            
            # Check only a narrow range between and around the found IDs (likely related files)
            search_range = range(min_id - 2, max_id + 3)
            
            for test_id in search_range:
                test_url = f"https://reproducibility.worldbank.org/index.php/catalog/{catalog_id}/download/{test_id}"
                try:
                    test_response = session.head(test_url, timeout=5)
                    if test_response.status_code == 200:
                        content_length = int(test_response.headers.get('content-length', '0'))
                        content_disposition = test_response.headers.get('content-disposition', '')
                        
                        # Extract filename to check if it's related to our DOI/study
                        filename = ""
                        if content_disposition and 'filename=' in content_disposition:
                            filename = content_disposition.split('filename=')[1].strip('"').strip("'").lower()
                        
                        # Only include files that appear to be related to our study
                        # Look for our DOI suffix or the study ID in the filename
                        study_indicators = [
                            doi_suffix.lower(),
                            'pp_civ_2025_368',  # The study ID from our redirect
                        ]
                        
                        is_related = any(indicator in filename for indicator in study_indicators if indicator)
                        
                        # Only include files that are clearly related to our study
                        if content_length > 100 and is_related:
                            all_download_ids.add(str(test_id))
                            if str(test_id) not in html_download_ids:
                                print(f"   Found additional file: ID {test_id} ({content_length:,} bytes) - {filename}")
                except:
                    # Ignore failed requests - file doesn't exist or isn't accessible
                    continue
        
        download_ids = sorted(all_download_ids, key=int)
        
        if not download_ids:
            print("DEBUG: Looking for 'download' in page content...")
            download_occurrences = [m.start() for m in re.finditer('download', page_content.lower())]
            print(f"Found 'download' at positions: {download_occurrences[:10]}...")  # Show first 10
            raise Exception("Could not find any download links")
        
        print(f"üîç Total download IDs found: {download_ids}")
        
        return catalog_id, download_ids
        
    except Exception as e:
        spinner.stop()
        raise Exception(f"Failed to resolve DOI: {e}")

def identify_file_type(url, headers=None):
    """Identify file type based on headers and heuristics."""
    if not headers:
        try:
            response = requests.head(url)
            headers = response.headers
        except:
            return "unknown", None
    
    content_type = headers.get('content-type', '').lower()
    content_disposition = headers.get('content-disposition', '')
    
    # Extract filename from Content-Disposition if available
    filename = ""
    if content_disposition and 'filename=' in content_disposition:
        filename = content_disposition.split('filename=')[1].strip('"').strip("'").lower()
    
    # Check file type based on filename first (more reliable than content-type)
    if filename.endswith('.pdf'):
        return "pdf", content_disposition
    elif filename.endswith('.zip'):
        return "zip", content_disposition
    elif filename.endswith(('.txt', '.md', '.readme')):
        return "text", content_disposition
    
    # Fallback to content type
    if 'application/pdf' in content_type:
        return "pdf", content_disposition
    elif 'application/zip' in content_type:
        return "zip", content_disposition
    elif 'text/plain' in content_type:
        return "text", content_disposition
    elif 'application/octet-stream' in content_type:
        # octet-stream is generic - try to guess from filename or context
        if 'readme' in filename or 'readme' in content_disposition.lower():
            if filename.endswith('.pdf'):
                return "pdf", content_disposition
            else:
                return "text", content_disposition
        elif filename.endswith('.zip') or 'zip' in content_disposition.lower():
            return "zip", content_disposition
        else:
            return "unknown", content_disposition
    
    return "unknown", content_disposition

def download_file(url, filepath, description="file"):
    """Download a file from URL."""
    print(f"‚¨áÔ∏è  Downloading: {description}")
    
    try:
        with requests.get(url, stream=True) as response:
            response.raise_for_status()
            
            # Get total file size if available
            total_size = int(response.headers.get('content-length', 0))
            
            spinner = Spinner(f"Downloading {description}", total_size)
            spinner.start()
            
            # Create parent directories if they don't exist
            filepath.parent.mkdir(parents=True, exist_ok=True)
            
            downloaded_size = 0
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    spinner.update_progress(downloaded_size)
        
        spinner.stop()
        
        # Show final download info
        is_ci = os.getenv("CI")
        final_info = f"Downloaded: {spinner.format_bytes(downloaded_size)}"
        if total_size > 0:
            final_info += f" of {spinner.format_bytes(total_size)} (100.0%)"
        
        if is_ci:
            print(f"{final_info}")
        else:
            print(f"‚úÖ {final_info}")
        
        return True
        
    except requests.exceptions.RequestException as e:
        if 'spinner' in locals():
            spinner.stop()
        print(f"‚ùå Error downloading {description}: {e}")
        return False

def unzip_file(zip_path, extract_to):
    """Extract a zip file."""
    print(f"üì¶ Unzipping: {zip_path.name}")
    
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
        print(f"‚úÖ Unzipped to: {extract_to}")
        return True
    except Exception as e:
        print(f"‚ùå Failed to unzip {zip_path.name}: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description='Download files from World Bank Reproducible Research Repository')
    parser.add_argument('doi_or_id', help='World Bank repository identifier (DOI suffix, DOI, or DOI URL)')
    parser.add_argument('--output', default='.', help='Output directory (default: current directory)')
    parser.add_argument('--dry-run', action='store_true', help='Show what would be downloaded without actually downloading')
    parser.add_argument('--version', action='version', version=f'%(prog)s {__version__}')
    
    args = parser.parse_args()
    
    # Print version as first line of output
    print(f"World Bank Download Script - Version {__version__}")
    
    try:
        # Extract DOI suffix from input
        doi_suffix = extract_doi_suffix(args.doi_or_id)
        print(f"üè∑Ô∏è  DOI suffix: {doi_suffix}")
    except ValueError as e:
        print(f"‚ùå Error: {e}")
        print("Expected formats:")
        print("  - DOI suffix: 101y-vn15")
        print("  - DOI URL: https://doi.org/10.60572/101y-vn15")
        print("  - DOI: 10.60572/101y-vn15")
        sys.exit(1)
    
    # Create output directory
    output_dir = Path(args.output) / f"wb-{doi_suffix}"
    if not args.dry_run:
        if output_dir.exists():
            print(f"‚ùå Error: {output_dir} already exists - please remove prior to downloading")
            sys.exit(1)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"üìÇ Output directory: {output_dir}")
    
    try:
        # Resolve catalog ID and discover download IDs
        catalog_id, download_ids = resolve_catalog_info(doi_suffix)
    except Exception as e:
        print(f"‚ùå {e}")
        sys.exit(1)
    
    # Base URL for downloads
    base_url = f"https://reproducibility.worldbank.org/index.php/catalog/{catalog_id}/download"
    
    # Current date for verification report filename
    current_date = datetime.now().strftime("%Y-%m-%d")
    
    print(f"\nüì¶ Found {len(download_ids)} file(s) to download:")
    
    if args.dry_run:
        for i, download_id in enumerate(download_ids, 1):
            file_url = f"{base_url}/{download_id}"
            try:
                response = requests.head(file_url)
                file_type, content_disposition = identify_file_type(file_url, response.headers)
                content_length = response.headers.get('content-length', 'Unknown')
                print(f"  {i}. Download ID {download_id}: {file_type} ({content_length} bytes)")
                if content_disposition:
                    print(f"     Content-Disposition: {content_disposition}")
            except Exception as e:
                print(f"  {i}. Download ID {download_id}: Error checking file - {e}")
        
        print(f"\nüîç Dry run completed. No files were downloaded.")
        return
    
    # Download and process each file
    downloaded_files = []
    failed_downloads = []
    pdf_count = 0  # Track PDF files to determine naming
    
    for i, download_id in enumerate(download_ids, 1):
        file_url = f"{base_url}/{download_id}"
        
        print(f"\nüìã Processing file {i}/{len(download_ids)} (Download ID: {download_id})")
        
        try:
            # Identify file type
            file_type, content_disposition = identify_file_type(file_url)
            
            # Determine filename based on Content-Disposition or file type
            if content_disposition and 'filename=' in content_disposition:
                # Extract filename from Content-Disposition header
                disp_filename = content_disposition.split('filename=')[1].strip('"').strip("'")
                if disp_filename.endswith('.pdf') and 'readme' in disp_filename.lower():
                    filename = "README.pdf"
                elif disp_filename.endswith('.pdf'):
                    filename = f"reproducibility-wb-{doi_suffix}.{current_date}.pdf"  
                elif disp_filename.endswith('.zip'):
                    filename = f"wb-{doi_suffix}.zip"
                else:
                    filename = disp_filename  # Use original filename
            elif file_type == "pdf":
                pdf_count += 1
                if pdf_count == 1:
                    filename = "README.pdf"
                else:
                    filename = f"reproducibility-wb-{doi_suffix}.{current_date}.pdf"
            elif file_type == "zip":
                filename = f"wb-{doi_suffix}.zip"
            else:
                # Default naming for unknown types
                filename = f"download-{download_id}"
            
            file_path = output_dir / filename
            
            print(f"üéØ File type: {file_type}, saving as: {filename}")
            
            # Download the file
            if download_file(file_url, file_path, filename):
                downloaded_files.append(filename)
                
                # If this is a zip file, unzip it
                if file_type == "zip":
                    if unzip_file(file_path, output_dir):
                        print(f"üí° You may want to remove the zip file: {filename}")
            else:
                failed_downloads.append(download_id)
                
        except Exception as e:
            print(f"‚ùå Error processing download ID {download_id}: {e}")
            failed_downloads.append(download_id)
    
    # Summary
    print(f"\nüéâ Download process completed!")
    print(f"‚úÖ Successfully downloaded: {len(downloaded_files)} files")
    
    if downloaded_files:
        print("üìÅ Downloaded files:")
        for filename in downloaded_files:
            print(f"  - {filename}")
    
    if failed_downloads:
        print(f"‚ùå Failed downloads: {len(failed_downloads)} files")
        print(f"   Download IDs: {failed_downloads}")
        sys.exit(1)
    
    print(f"\nüìÇ All files saved to: {output_dir}")
    print(f"\nüìã Contents of {output_dir}:")
    try:
        for item in sorted(output_dir.iterdir()):
            if item.is_file():
                size = item.stat().st_size
                print(f"  üìÑ {item.name} ({size:,} bytes)")
            elif item.is_dir():
                print(f"  üìÅ {item.name}/")
    except Exception:
        print("  (Could not list directory contents)")

if __name__ == '__main__':
    main()